
# Pipeline_function --------------------------------------------------------

#' @title AutoScore-Ordinal STEP (1): Generate variable ranking List by machine
#'   learning (AutoScore-Ordinal Module 1)
#' @param train_set A processed \code{data.frame} that contains data to be
#'   analyzed, for training. MIssing values and outliers should be cleaned
#'   first. The outcome's name should be "label". Going through
#'   \code{Preprocess} for preprocessing data and \code{\link{split_data}} for
#'   splitting data, before AutoScore-Ordinal implementation, if needed.
#' @param ntree Number of trees in random forest algorithm for variable ranking,
#'   default:100
#'
#' @details The first step in the AutoScore-Ordinal framework is variable
#'   ranking. We use random forest (RF), an ensemble machine learning algorithm,
#'   to identify the top-ranking predictors for subsequent score generation.
#'   This step correspond to Module 1 in the AutoScore-Ordinal paper, which is
#'   modified from the AutoScore paper.
#'
#' @return Returns a vector containing the list of variables and its ranking
#'   generated by machine learning (random forest)
#'
#' @examples
#' # See AutoScore-Ordinal Guidebook for the whole 5-step workflow
#' \dontrun{
#' ranking <- AutoScoreOrdinal_rank(train_set, ntree=100)
#' }
#'
#' @references
#' \itemize{
#'  \item{Breiman, L. (2001), Random Forests, Machine Learning 45(1), 5-32}
#'  \item{Xie F, Chakraborty B, Ong MEH, Goldstein BA, Liu N, AutoScore: A
#'  Machine Learning-Based Automatic Clinical Score Generator and Its
#'  Application to Mortality Prediction Using Electronic Health Records, JMIR
#'  Med Inform 2020;8(10):e21798, doi: 10.2196/21798}
#' }
#' @import randomForest
#' @export
AutoScoreOrdinal_rank <- function(train_set, ntree = 100) {
  train_set$label <- ordered(train_set$label) # Ordered factor
  model <- randomForest::randomForest(label ~ ., data = train_set, ntree = ntree,
                                      preProcess = "scale")

  # estimate variable importance
  importance <- randomForest::importance(model, scale = F)

  # summarize importance
  names(importance) <- rownames(importance)
  importance <- sort(importance, decreasing = T)
  cat("The ranking based on variable importance was shown below for each variable: \n")
  print(importance)
  return(importance)
}


#' @title AutoScoreOrdinal STEP(ii): Select the best model with parsimony plot
#'   (AutoScore Modules 2+3+4)
#' @param train_set A processed \code{data.frame} that contains data to be
#'   analyzed, for training.
#' @param validation_set A processed \code{data.frame} that contains data for
#'   validation purpose.
#' @param rank the raking result generated from AutoScore STEP(i)
#'   \code{AutoScore_rank}
#' @param n_min Minimum number of selected variables (Default: 1).
#' @param n_max Maximum number of selected variables (Default: 20).
#' @param max_score Maximum total score (Default: 100).
#' @param cross_validation If set to \code{TRUE}, cross-validation would be used
#'   for generating parsimony plot, which is suitable for small-size data.
#'   Default to \code{FALSE}
#' @param fold The number of folds used in cross validation (Default: 10).
#'   Available if cross_validation = TRUE.
#' @param categorize  Methods for categorize continuous variables. Options
#'   include "quantile" or "kmeans" (Default: "quantile").
#' @param quantiles Predefined quantiles to convert continuous variables to
#'   categorical ones. (Default: c(0, 0.05, 0.2, 0.8, 0.95, 1)) Available if
#'   \code{categorize = "quantile"}.
#' @param max_cluster The max number of cluster (Default: 5). Available if
#'   categorize = "kmeans".
#' @param do_trace If set to TRUE, all results based on each fold of
#'   cross-validation would be printed out and plotted (Default: FALSE).
#'   Available if cross_validation = TRUE.
#'
#' @details This is the second step of the general AutoScore workflow, to
#'   generate the parsimony plot to help select parsimonious model. In this
#'   step, it goes through AutoScore Module 2,3 and 4 multiple times and to
#'   evaluate the performance under different variable list. The generated
#'   parsimony plot would give researcher an intuitive figure to choose the best
#'   models. If data size is small (ie <5000), an independent validation set may
#'   not be a wise choice. Then we will use cross-validation to maximize the
#'   utility of data. Set \code{cross_validation=TRUE}.
#'
#' @return List of AUC value for different number of variables
#'
#' @examples
#' # see AutoScore Guidebook for the whole 5-step workflow
#' data("sample_data")
#' names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
#' out_split <- split_data(data = sample_data, ratio = c(0.7, 0.1, 0.2))
#' train_set <- out_split$train_set
#' validation_set <- out_split$validation_set
#' ranking <- AutoScore_rank(train_set, ntree=100)
#' AUC <- AutoScore_parsimony(
#' train_set,
#' validation_set,
#' rank = ranking,
#' max_score = 100,
#' n_min = 1,
#' n_max = 20,
#' categorize = "quantile",
#' quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1)
#' )
#'
#' @references
#' \itemize{
#'  \item{Xie F, Chakraborty B, Ong MEH, Goldstein BA, Liu N, AutoScore: A
#'  Machine Learning-Based Automatic Clinical Score Generator and Its
#'  Application to Mortality Prediction Using Electronic Health Records, JMIR
#'  Med Inform 2020;8(10):e21798, doi: 10.2196/21798}
#' }
#' @seealso AutoScore_rank, AutoScore_parsimony, AutoScore_weighting,
#' AutoScore_fine_tuning, AutoScore_testing
#' @export
#' @import  pROC
AutoScoreOrdinal_parsimony <- function(train_set, validation_set, rank,
                                       max_score = 100, n_min = 1, n_max = 20,
                                       cross_validation = FALSE, fold = 10,
                                       categorize = "quantile",
                                       quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1),
                                       max_cluster = 5,
                                       do_trace = FALSE) {
  if (n_max > length(rank)) {
    warning("WARNING: the n_max (", n_max, ") is larger the number of all variables (",
            length(rank), "). We Automatically revise the n_max to ", length(rank))
    n_max <- length(rank)
  }
  # Cross Validation scenario
  if (cross_validation == TRUE) {
    # Divide the data equally into n fold, record its index number
    set.seed(4)
    index <- list()
    all <- 1:length(train_set[, 1])
    for (i in 1:(fold - 1)) {
      a <- sample(all, trunc(length(train_set[, 1]) / fold))
      index <- append(index, list(a))
      all <- all[!(all %in% a)]
    }
    index <- c(index, list(all))

    # Create a new variable auc_set to store all AUC value during the corss-validation
    auc_set <- data.frame(rep(0, n_max - n_min + 1))

    # for each fold, generate train_set and validation_set
    for (j in 1:fold) {
      validation_set_temp <- train_set[index[[j]], ]
      train_set_tmp <- train_set[-index[[j]], ]

      #variable_list <- names(rank)
      AUC <- c()

      # Go through AUtoScore Module 2/3/4 in the loop
      for (i in n_min:n_max) {
        variable_list <- names(rank)[1:i]
        train_set_1 <- train_set_tmp[, c(variable_list, "label")]
        validation_set_1 <- validation_set_temp[, c(variable_list, "label")]

        model_mean_auc <- compute_auc_val(train_set_1 = train_set_1,
                                          validation_set_1 = validation_set_1,
                                          variable_list = variable_list,
                                          categorize = categorize,
                                          quantiles = quantiles,
                                          max_cluster = max_cluster,
                                          max_score = max_score)
        AUC <- c(AUC, model_mean_auc$auc[is.na(model_mean_auc$j)])
      }

      # plot parsimony plot for each fold
      names(AUC) <- n_min:n_max

      # only print and plot when do_trace = TRUE
      if(do_trace){
        print(paste("list of AUC values for fold",j))
        print(data.frame(AUC))
        plot(AUC, main = paste("Parsimony plot (cross validation) for fold",j), xlab = "Number of Variables", ylab = "Area Under the Curve", col = "red",
             lwd = 2, type = "o")}

      # store AUC result from each fold into "auc_set"
      auc_set<-cbind(auc_set,data.frame(AUC))

    }

    # finish loop and then output final results averaged by all folds
    auc_set$rep.0..n_max...n_min...1.<-NULL
    auc_set$sum<-rowSums(auc_set)/fold
    cat("list of fianl AUC values through cross validation are shown below")
    print(data.frame(auc_set$sum))
    plot(auc_set$sum, main = paste("Final Parsimony plot based on ", fold, "-fold Cross Validation",sep = ""), xlab = "Number of Variables", ylab = "Area Under the Curve", col = "red",
         lwd = 2, type = "o")
    return(auc_set)
  } else {# Go through AUtoScore Module 2/3/4 in the loop
    perform_stat <- do.call("rbind", lapply(n_min:n_max, function(i) {
      cat("Select", i, "variables:  ")
      variable_list <- names(rank)[1:i]
      train_set_1 <- train_set[, c(variable_list, "label")]
      validation_set_1 <- validation_set[, c(variable_list, "label")]
      model_auc <- compute_auc_val(train_set_1 = train_set_1,
                                   validation_set_1 = validation_set_1,
                                   variable_list = variable_list,
                                   categorize = categorize,
                                   quantiles = quantiles,
                                   max_cluster = max_cluster,
                                   max_score = max_score)
      cumm_auc_i <- model_auc$auc[is.na(model_auc$j)]
      cat(cumm_auc_i, "\n")
      cbind(m = i, model_auc)
    }))
    # names(vus) <- n_min:n_max
    plot(x = n_min:n_max, y = perform_stat$auc[is.na(perform_stat$j)],
         main = "Parsimony plot on the Validation Set",
         xlab = "Number of Variables", ylab = "Mean Area Under the Curve",
         col = "steelblue", lwd = 2, type = "o")
    perform_stat
  }
}
#' @title AutoScore STEP(iii): Generate the initial score with the final list of variables (Re-run AutoScore Modules 2+3)
#' @param train_set A processed \code{data.frame} that contains data to be analyzed, for training.
#' @param validation_set A processed \code{data.frame} that contains data for validation purpose.
#' @param final_variables A vector containing the list of selected variables, selected from Step(2). See Guidebook
#' @param max_score Maximum total score (Default: 100).
#' @param categorize  Methods for categorize continuous variables. Options include "quantile" or "kmeans" (Default: "quantile").
#' @param quantiles Predefined quantiles to convert continuous variables to categorical ones. (Default: c(0, 0.05, 0.2, 0.8, 0.95, 1)) Available if \code{categorize = "quantile"}.
#' @param max_cluster The max number of cluster (Default: 5). Available if categorize = "kmeans".
#' @return Generated \code{cut_vec} for downstream fine-tuning process [STEP(iv)]
#' @examples
#' data("sample_data")
#' names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
#' out_split <- split_data(data = sample_data, ratio = c(0.7, 0.1, 0.2))
#' train_set <- out_split$train_set
#' validation_set <- out_split$validation_set
#' ranking <- AutoScore_rank(train_set, ntree=100)
#' num_var <- 6
#' final_variables <- names(ranking[1:num_var])
#' cut_vec <- AutoScore_weighting(
#' train_set,
#' validation_set,
#' final_variables,
#' max_score = 100,
#' categorize = "quantile",
#' quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1)
#' )
#' @references
#' \itemize{
#'  \item{Xie F, Chakraborty B, Ong MEH, Goldstein BA, Liu N. AutoScore: A Machine Learning-Based Automatic Clinical Score Generator and
#'   Its Application to Mortality Prediction Using Electronic Health Records. JMIR Medical Informatics 2020;8(10):e21798}
#' }
#' @export
#' @import knitr
AutoScoreOrdinal_weighting <- function(train_set, validation_set, final_variables,
                                       max_score = 100,
                                       categorize = "quantile",
                                       quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1),
                                       max_cluster = 5) {
  # prepare train_set and validation_set
  cat("****Included Variables: \n")
  print(data.frame(variable_name = final_variables))
  train_set_1 <- train_set[, c(final_variables, "label")]
  validation_set_1 <- validation_set[, c(final_variables, "label")]

  # AutoScore Module 2 : cut numeric and transform categories and generate "cut_vec"
  cut_vec <- get_cut_vec(df = train_set_1, categorize = categorize,
                         quantiles = quantiles, max_cluster = max_cluster)
  train_set_2 <- transform_df_fixed(df = train_set_1, cut_vec = cut_vec)
  validation_set_2 <- transform_df_fixed(df = validation_set_1, cut_vec = cut_vec)

  # AutoScore Module 3 : Score weighting
  score_list <- compute_score_table(train_set_2 = train_set_2,
                                    validation_set_2 = validation_set_2,
                                    max_score = max_score,
                                    variable_list = final_variables)
  cat("****Initial Scores: \n")
  print_scoring_table(scoring_table = score_list$score_table,
                      final_variable = final_variables)
  # Using "auto_test" to generate score based on new dataset and Scoring table "score_table"
  validation_set_3 <- assign_score(df = validation_set_2,
                                   score_table = score_list$score_table)
  validation_set_3$total_score <- rowSums(subset(validation_set_3,
                                                 select = final_variables))

  # Intermediate evaluation based on Validation Set
  model_auc <- compute_mean_auc(y = validation_set_3$label,
                                fx = validation_set_3$total_score)
  cat("***Performance (based on validation set):\n")
  print(knitr::kable(model_auc), digits = 3)
  cat("***The cutoffs of each variables generated by the AutoScoreOrdinal are saved in cut_vec. You can decide whether to revise or fine-tune them \n")
  return(cut_vec)
}


#' @title AutoScore STEP(iv): Fine-tune the score by revising cut_vec with domain knowledge (AutoScore Module 5)
#' @description Domain knowledge is essential in guiding risk model development.
#'  For continuous variables, the variable transformation is a data-driven process (based on "quantile" or "kmeans" ).
#'  In this step, the automatically generated cutoff values for each continuous variable can be fine-tuned
#'  by combining, rounding, and adjusting according to the standard clinical norm.  Revised \code{cut_vec} will be input  with domain knowledge to
#' update scoring table. User can choose any cut-off values/any number of categories. Then final Scoring table will be generated. See Guidebook.
#'
#' @param train_set A processed \code{data.frame} that contains data to be analyzed, for training.
#' @param validation_set A processed \code{data.frame} that contains data for validation purpose.
#' @param final_variables A vector containing the list of selected variables, selected from Step(2). See Guidebook
#' @param max_score Maximum total score (Default: 100).
#' @param cut_vec Generated from STEP(iii) \code{AutoScore_weighting()}.Please follow the guidebook
#'
#' @return Generated final table of scoring model for downstream testing
#' @examples
#' \dontrun{
#' scoring_table <- AutoScore_fine_tuning(train_set, validation_set,
#'  final_variables, cut_vec, max_score = 100)}
#' @references
#' \itemize{
#'  \item{Xie F, Chakraborty B, Ong MEH, Goldstein BA, Liu N. AutoScore: A Machine Learning-Based Automatic Clinical Score Generator and
#'   Its Application to Mortality Prediction Using Electronic Health Records. JMIR Medical Informatics 2020;8(10):e21798}
#' }
#' @import pROC
#' @import ggplot2
#' @export
AutoScoreOrdinal_fine_tuning <- function(train_set, validation_set, final_variables,
                                         cut_vec, max_score = 100) {
  # Prepare train_set and VadalitionSet
  train_set_1 <- train_set[, c(final_variables, "label")]
  validation_set_1 <- validation_set[, c(final_variables, "label")]

  # AutoScore Module 2 : cut numeric and transfer categories (based on fix "cut_vec" vector)
  train_set_2 <- transform_df_fixed(df = train_set_1, cut_vec = cut_vec)
  validation_set_2 <- transform_df_fixed(df = validation_set_1, cut_vec = cut_vec)

  # AutoScore Module 3 : Score weighting
  score_list <- compute_score_table(train_set_2 = train_set_2,
                                    validation_set_2 = validation_set_2,
                                    max_score = max_score,
                                    variable_list = final_variables)
  # Revised scoring table representation if possible @YuanHan
  cat("***Fine-tuned Scores: \n")
  print_scoring_table(scoring_table = score_list$score_table,
                      final_variable = final_variables)

  # Using "auto_test" to generate score based on new dataset and Scoring table "score_table"
  validation_set_3 <- assign_score(df = validation_set_2,
                                   score_table = score_list$score_table)
  validation_set_3$total_score <- rowSums(subset(validation_set_3, select = -label))

  # Intermediate evaluation based on Validation Set after fine-tuning
  model_auc <- compute_mean_auc(y = validation_set_3$label,
                                fx = validation_set_3$total_score)
  cat("***Performance (based on Validation Set, after fine-tuning):\n")
  print(knitr::kable(model_auc, digits = 3))
  return(score_list)
}


#' @title AutoScore STEP(v): Evaluate the final score with ROC analysis (AutoScore Module 6)
#' @description Domain knowledge is essential in guiding risk model development.
#'  For continuous variables, the variable transformation is a data-driven process (based on "quantile", "kmeans" or "decision_tree).
#'  In this step, the automatically generated cutoff values for each continuous variable can be fine-tuned
#'  by combining, rounding, and adjusting according to the standard clinical norm.  Revised \code{cut_vec} will be input  with domain knowledge to
#' update scoring table. User can choose any cut-off values/any number of categories. Then final Scoring table will be generated. See Guidebook.
#'
#' @param test_set A processed \code{data.frame} that contains data for testing purpose. This \code{data.frame} should have same format as
#'        \code{train_set} (same variable names and outcomes)
#' @param final_variables A vector containing the list of selected variables, selected from Step(ii). See Guidebook
#' @param scoring_table The final scoring table after fine-tuning, generated from STEP(iv) \code{AutoScore_fine_tuning}.Please follow the guidebook
#' @param cut_vec Generated from STEP(iii) \code{AutoScore_weighting()}.Please follow the guidebook
#' @param threshold Score threshold for the ROC analysis to generate sensitivity, specificity, etc. If set to "best", the optimal threshold will be calculated (Default:"best").
#' @param with_label Set to TRUE if there are labels in the test_set and performance will be evaluated accordingly (Default:TRUE).
#'
#' @return A data frame with predicted score and the outcome for downstream visualization.
#' @examples
#' \dontrun{
#' pred_score <- AutoScore_testing(test_set, final_variables, cut_vec,
#' scoring_table, threshold = "best", with_label = TRUE)
#' }
#' @references
#' \itemize{
#'  \item{Xie F, Chakraborty B, Ong MEH, Goldstein BA, Liu N. AutoScore: A Machine Learning-Based Automatic Clinical Score Generator and
#'   Its Application to Mortality Prediction Using Electronic Health Records. JMIR Medical Informatics 2020;8(10):e21798}
#' }
#' @import pROC
#' @import ggplot2
#' @export
AutoScoreOrdinal_testing <- function(test_set, final_variables, cut_vec,
                                     score_table, score_thresholds) {
  # prepare testset: categorization and "auto_test"
  test_set_1 <- test_set[, c(final_variables, "label")]
  test_set_2 <- transform_df_fixed(df = test_set_1, cut_vec = cut_vec)
  test_set_3 <- assign_score(df = test_set_2, score_table = score_table)
  test_set_3$total_score <- rowSums(subset(test_set_3, select = final_variables))
  if (anyNA(test_set_3$total_score)) {
    warning(simpleWarning("NA in risk score. Check analyses."))
    test_set_3$total_score[which(is.na(test_set_3$total_score))] <- 0
  }
  y_test <- test_set_3$label
  y_pred <- cut(x = test_set_3$total_score,
                breaks = c(-Inf, score_thresholds, Inf),
                labels = 1:(length(score_thresholds) + 1))
  mae <- mean(abs(as.numeric(y_pred) - as.numeric(y_test)))

  # Final evaluation based on testing set
  model_auc <- compute_mean_auc(y = y_test, fx = test_set_3$total_score)
  cat("***Performance using AutoScore (based on unseen test Set):\n")
  print(knitr::kable(model_auc, digits = 3))
  cat(sprintf("MAE = %.3f\n", mae))
  return(c(AUC = model_auc, MAE = mae))
}

# Direct_function ---------------------------------------------------------

#' @title AutoScore function: Check whether the input dataset fulfill the requirement of the AutoScore
#' @param data The data to be checked
#' @examples
#' data("sample_data")
#' names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
#' check_data(sample_data)
#' @export
check_data<-function(data){
  #1. check label and binary
  if (is.null(data$label)) {
    stop("ERROR for this dataset: These is no dependent variable -lable- to indicate the outcome. Please add one first")
  }
  if (!is.factor(data$label)) {
    stop("ERROR for this dataset: Dependent variable should be an ordered factor. Please convert it using function 'ordered'")
  }

  #2. check each variable
  non_num_fac <- c()
  fac_large <- c()
  special_case <- c()

  for (i in setdiff(names(data), "label")) { # No longer need to check outcome
    if ((class(data[[i]]) != "factor") && (class(data[[i]]) != "numeric")) {
      non_num_fac <- c(non_num_fac, i)
    }
    if ((length(levels(data[[i]])) > 10) && (class(data[[i]]) == "factor")) {
      fac_large <- c(fac_large, i)
    }
    if (grepl(",", i)) {
      warning(paste("WARNING: the dataset has variable names", i,
                    "with character , please change it"))
    }
    if (grepl(")", i)) {
      warning(paste("WARNING: the dataset has variable names", i,
                    "with character ) please change it"))
    }
  }

  if (!is.null(non_num_fac)) {
    warning(paste("WARNING: the dataset has variable of character and they will be transformed to factor:", non_num_fac))
  }
  if (!is.null(fac_large)) {
    warning(paste("WARNING: The number of categories for some variables is too many :larger than:", fac_large))
  }

  #3. check missing values
  missing_rate <- colSums(is.na(data))
  if (sum(missing_rate)) {
    warning("WARNING: Your dataset contains NA. Please handle them before AutoScore. The variables with missing values are shown below:")
    print(missing_rate[missing_rate != 0])
  } else cat("missing value check passed.\n")
}


#' @title AutoScoreOrdinal Function: Automatically splitting dataset to train,
#'   validation and test set
#' @param data The dataset to be split
#' @param ratio The ratio to divide dataset into train, validation and test set.
#'   (Default: c(0.7, 0.1, 0.2))
#' @param cross_validation If set to \code{TRUE}, cross-validation would be used
#'   for generating parsimony plot, which is suitable for small-size data.
#'   Default to \code{FALSE}
#' @return Returns a list containing training, validation and testing set
#' @examples
#' data("sample_data")
#' set.seed(4)
#' #large sample size
#' out_split <- split_data(data = sample_data, ratio = c(0.7, 0.1, 0.2))
#' #small sample size
#' out_split <- split_data(data = sample_data, ratio = c(0.7, 0, 0.3), cross_validation = TRUE)
#' @export
split_data <- function(data, ratio, cross_validation = FALSE) {
  # non cross validation: default
  if (cross_validation == FALSE) {
    n <- length(data[, 1])
    test_ratio <- ratio[3] / sum(ratio)
    validation_ratio <- ratio[2] / sum(ratio)
    test_index <- sample((1:n), test_ratio * n)
    validate_index <- sample((1:n)[!(1:n) %in% test_index], validation_ratio * n)
    train_set <- data[-c(validate_index, test_index), ]
    test_set <- data[test_index, ]
    validation_set <- data[validate_index, ]

    return(list(train_set = train_set, validation_set = validation_set,
                test_set = test_set))
  } else {# cross validation: train = validation
    n <- length(data[, 1])
    test_ratio <- ratio[3] / sum(ratio)
    validation_ratio <- ratio[2] / sum(ratio)
    test_index <- sample((1:n), test_ratio * n)
    validate_index <- sample((1:n)[!(1:n) %in% test_index], validation_ratio * n)
    train_set <- data[-c(test_index),]
    test_set <- data[test_index,]
    validation_set <- train_set

    return(list(train_set = train_set, validation_set = validation_set,
                test_set = test_set))
  }
}


#' @title AutoScoreOrdinal function: Descriptive Analysis
#' @description Compute descriptive table (usually Table 1 in medical
#'   literature) for the dataset.
#' @param df data frame after checking
#' @examples
#' data("sample_data")
#' names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
#' compute_descriptive_table(sample_data)
#' @export
#' @import tableone
compute_descriptive_table <- function(df) {
  descriptive_table <- CreateTableOne(vars = names(df), strata = "label", data = df)
  descriptive_table_overall <- CreateTableOne(vars = names(df), data = df)
  print(descriptive_table)
  print(descriptive_table_overall)
}
#' @title AutoScoreOrdinal function: Univariable Analysis
#' @description Perform univariable analysis and generate the result table with odd ratios.
#' @param df data frame after checking
#' @return Result of univariable analysis
#' @examples
#' data("sample_data")
#' names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
#' uni_table<-compute_uni_variable_table(sample_data)
#' @import ordinal
#' @export
compute_uni_variable_table <- function(df) {
  J <- length(unique(df$label))
  uni_table <- do.call("rbind", lapply(setdiff(names(df), "label"), function(i) {
    model <- clm(as.formula("label ~ ."), data = subset(df, select = c("label", i)),
                 link = "logit", na.action = na.omit)
    mat <- cbind(exp(cbind(OR = coef(model), confint.default(model))),
                 summary(model)$coef[, "Pr(>|z|)"])
    mat_i <- matrix(mat[-(1:(J - 1)), ], ncol = 4)
    colnames(mat_i) <- colnames(mat)
    rownames(mat_i) <- names(model$beta)
    mat_i
  }))
  uni_table <- as.data.frame(uni_table)
  uni_table <- round(uni_table, digits = 3)
  uni_table$V4[uni_table$V4 < 0.001] <- "<0.001"
  uni_table$OR <- paste0(uni_table$OR, "(", uni_table$`2.5 %`, "-",
                         uni_table$`97.5 %`, ")")
  uni_table$`2.5 %` <- NULL
  uni_table$`97.5 %` <- NULL
  names(uni_table)[names(uni_table) == "V4"] <- "p value"
  return(uni_table)
}
#' @title AutoScore function: Multivariate Analysis
#' @description Generate tables for Multivariate Analysis
#' @param df data frame after checking
#' @return result of Multivariate Analysis
#' @examples
#' data("sample_data")
#' names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
#' multi_table<-compute_multi_variable_table(sample_data)
#' @export
# full logistic model-multivariable analysis
compute_multi_variable_table <- function(df) {
  J <- length(unique(df$label))
  model <- clm(as.formula("label ~ ."), data = df,
               link = "logit", na.action = na.omit)
  mat <- cbind(exp(cbind(adjusted_OR = coef(model), confint.default(model))),
               summary(model)$coef[, "Pr(>|z|)"])
  multi_table <- matrix(mat[-(1:(J - 1)), ], ncol = 4)
  colnames(multi_table) <- colnames(mat)
  rownames(multi_table) <- names(model$beta)
  multi_table <- as.data.frame(multi_table)
  multi_table <- round(multi_table, digits = 3)
  multi_table$V4[multi_table$V4 < 0.001] <- "<0.001"
  multi_table$adjusted_OR <- paste0(multi_table$adjusted_OR,
                                    "(", multi_table$`2.5 %`,
                                    "-", multi_table$`97.5 %`, ")")
  multi_table$`2.5 %` <- NULL
  multi_table$`97.5 %` <- NULL
  names(multi_table)[names(multi_table) == "V4"] <- "p value"
  return(multi_table)
}


#' @title AutoScore Function: print_scoring_table
#' @description Print scoring tables for visulization
#' @param scoring_table Raw scoring table generated by procedure
#' @param final_variable Final included variables by predictor_rank
#' @return Data frame of formatted scoring table
#' @examples
#' \dontrun{
#' print_scoring_table(scoring_table,final_variable)}
#' @export
#' @importFrom knitr kable
print_scoring_table<-function(scoring_table,final_variable){
  #library(knitr)
  table_tmp <- data.frame()
  for (i in 1:length(final_variable)) {
    var_tmp<-final_variable[i]
    var_name<-names(scoring_table)
    num<-grepl(var_tmp,var_name)
    table_1<-data.frame(name=var_name[num],value=unname(scoring_table[num]))
    rank_indicator<-gsub(".*,","",table_1$name)
    rank_indicator<-gsub(")","",rank_indicator)
    if (grepl(",",table_1$name[1])!=TRUE){
      table_1$rank_indicator<-c(seq(1:nrow(table_1)))
      interval<-c(gsub(pattern = var_tmp, replacement = "", table_1$name))
      table_1$interval<-interval
      table_2<-table_1[order(table_1$interval),]
      table_2$variable<-c(var_tmp,rep("",(nrow(table_2)-1)))
      table_3<-rbind(table_2,rep("",ncol(table_2)))
      table_tmp<-rbind(table_tmp,table_3)
    } else {
      rank_indicator[which(rank_indicator=="")]<-max(as.numeric(rank_indicator[-which(rank_indicator=="")]))+1
      rank_indicator<-as.numeric(rank_indicator)
      if (length(rank_indicator)==2) {
        table_1$rank_indicator<-rank_indicator
        table_2<-table_1[order(table_1$rank_indicator),]
        interval<-c(paste0("<",table_2$rank_indicator[1]))
        interval<-c(interval,paste0(">=",table_2$rank_indicator[length(rank_indicator)-1]))
        table_2$interval<-interval
        table_2$variable<-c(var_tmp,rep("",(nrow(table_2)-1)))
        table_3<-rbind(table_2,rep("",ncol(table_2)))
        table_tmp<-rbind(table_tmp,table_3)
      } else {
        table_1$rank_indicator<-rank_indicator
        table_2<-table_1[order(table_1$rank_indicator),]
        interval<-c(paste0("<",table_2$rank_indicator[1]))
        for (j in 1:(length(table_2$rank_indicator)-2)) {
          interval<-c(interval,paste0("[",table_2$rank_indicator[j],",",table_2$rank_indicator[j+1],")"))
        }
        interval<-c(interval,paste0(">=",table_2$rank_indicator[length(rank_indicator)-1]))
        table_2$interval<-interval
        table_2$variable<-c(var_tmp,rep("",(nrow(table_2)-1)))
        table_3<-rbind(table_2,rep("",ncol(table_2)))
        table_tmp<-rbind(table_tmp,table_3)
      }
    }
  }
  table_tmp<-table_tmp[1:(nrow(table_tmp)-1),]
  table_final<-data.frame(variable = table_tmp$variable,interval = table_tmp$interval, point = table_tmp$value)
  table_kable_format<-kable(table_final,align = "llc",caption = "AutoScore-created scoring model",format = "rst")
  print(table_kable_format)
  invisible(table_final)
}

# Build_in_function -------------------------------------------------------


#' Compute scoring table
#' @param max_score Restrict the largest possible score to avoid large partial
#'   scores that are difficult to use in practice. Set to \code{NULL} to not
#'   pose any restriction.
#' @import ordinal
compute_score_table <- function(train_set_2, validation_set_2, max_score,
                                variable_list) {
  #AutoScore Module 3 : Score weighting
  # First-step ordinal logistic regression
  model <- ordinal::clm(as.formula("label ~ ."), link = "logit", data = train_set_2)
  if (anyNA(model$alpha)) {
    stop(" Error: Ordinal regression produced invalid threshold. Check data.")
  }
  coef_vec <- model$beta
  if (anyNA(coef_vec)) {
    warning(" WARNING: Ordinal regression output contains NULL, Replace NULL with 1")
    coef_vec[which(is.na(coef_vec))] <- 1
  }
  train_set_2 <- change_reference(df = train_set_2, coef_vec = coef_vec)

  # Second-step ordinal logistic regression
  model <- ordinal::clm(as.formula("label ~ ."), link = "logit", data = train_set_2)
  coef_vec <- model$beta
  if (anyNA(coef_vec) > 0) {
    warning(" WARNING: GLM output contains NULL, Replace NULL with 1")
    coef_vec[which(is.na(coef_vec))] <- 1
  }
  theta_vec <- model$alpha
  thresholds <- compute_cutoffs(theta = theta_vec, link = "logit")

  # rounding for final scoring table "score_table"
  score_table <- add_baseline(train_set_2, round(coef_vec / min(coef_vec)))
  score_thresholds <- round(thresholds / min(coef_vec))

  if (!is.null(max_score)) {
    # normalization according to "max_score" and regenerate score_table
    total_max <- max_score
    total <- 0
    for (i in 1:length(variable_list)) {
      total <- total + max(score_table[grepl(variable_list[i], names(score_table))])
    }
    score_table <- round(score_table / (total / total_max))
    score_thresholds <- round(score_thresholds / (total / total_max))
  }
  return(list(score_table = score_table, score_thresholds = score_thresholds))
}


#' Compute mean AUC for ordinal predictions
#' @param y An ordered factor representing the ordinal outcome, with length n
#'   and J categories.
#' @param fx Either (i) a numeric vector of predictor (e.g., predicted scores)
#'   of length n or (ii) a numeric matrix of predicted cumulative probabilities
#'   with n rows and (J-1) columns.
#' @return A \code{data.frame} with the cumulative AUCs (i.e., when evaluating
#'   the prediction of Y<=j, j=1,...,J-1) and 95% CI of AUC and the mean of
#'   cumulative AUCs. The SE of the mean AUC is approximated by assuming
#'   independence of cumulative AUCs.
#' @import pROC
compute_mean_auc <- function(y, fx) {
  y <- as.numeric(y)
  J <- length(unique(y))
  if (is.null(dim(fx))) {# fx is a vector of linear predictor
    auc_df <- do.call("rbind", lapply(1:(J - 1), function(j) {
      model_roc <- pROC::roc(response = y <= j, predictor = fx, quiet = TRUE)
      auc_ci_vec <- pROC::ci.auc(model_roc)
      data.frame(j = j, auc = auc_ci_vec[2],
                 se = (auc_ci_vec[3] - auc_ci_vec[1]) / (1.96 * 2),
                 auc_lower = auc_ci_vec[1], auc_upper = auc_ci_vec[3])
    }))
  } else {# fx is a matrix with J-1 columns
    fx <- as.matrix(fx)
    if (nrow(fx) != length(y) | ncol(fx) != J - 1) {
      stop(simpleError("Wrong dimention for fx."))
    }
    auc_df <- do.call("rbind", lapply(1:(J - 1), function(j) {
      model_roc <- pROC::roc(response = y <= j, predictor = fx[, j], quiet = TRUE)
      auc_ci_vec <- pROC::ci.auc(model_roc)
      data.frame(j = j, auc = auc_ci_vec[2],
                 se = (auc_ci_vec[3] - auc_ci_vec[1]) / (1.96 * 2),
                 auc_lower = auc_ci_vec[1], auc_upper = auc_ci_vec[3])
    }))
  }
  # Approximate the SE of average AUC:
  se_mean_auc <- sqrt(sum(auc_df$se ^ 2) / nrow(auc_df))
  rbind(auc_df, data.frame(j = NA, auc = mean(auc_df$auc), se = se_mean_auc,
                           auc_lower = mean(auc_df$auc) - 1.96 * se_mean_auc,
                           auc_upper = mean(auc_df$auc) + 1.96 * se_mean_auc))
}
#' @import pROC
compute_auc_val <- function(train_set_1, validation_set_1, variable_list,
                            categorize, quantiles, max_cluster, max_score) {
  # AutoScore Module 2 : cut numeric and transfer categories
  cut_vec <- get_cut_vec(df = train_set_1, categorize = categorize,
                         quantiles = quantiles, max_cluster = max_cluster)
  train_set_2 <- transform_df_fixed(df = train_set_1, cut_vec = cut_vec)
  validation_set_2 <- transform_df_fixed(df = validation_set_1, cut_vec = cut_vec)
  if (sum(is.na(validation_set_2)) > 0) {
    warning("NA in the validation_set_2: ", sum(is.na(validation_set_2)))
  }
  if (sum(is.na(train_set_2)) > 0) {
    warning("NA in the train_set_2: ", sum(is.na(train_set_2)))
  }

  # AutoScore Module 3 : Variable Weighting
  score_list <- compute_score_table(train_set_2 = train_set_2,
                                    validation_set_2 = validation_set_2,
                                    variable_list = variable_list,
                                    max_score = max_score)

  # Using "auto_test" to generate score based on new dataset and Scoring table "score_table"
  validation_set_3 <- assign_score(df = validation_set_2,
                                   score_table = score_list$score_table)
  if (sum(is.na(validation_set_3)) > 0) {
    warning("NA in the validation_set_3: ", sum(is.na(validation_set_3)))
  }
  validation_set_3$total_score <- rowSums(subset(
    validation_set_3,
    select = setdiff(names(validation_set_3), "label")
  ))

  compute_mean_auc(y = validation_set_3$label, fx = validation_set_3$total_score)
}
get_cut_vec <- function(df, quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1),
                        max_cluster = 5, categorize = "quantile") {

  # Generate cut_vec for downstream usage
  cut_vec <- list()

  for (i in 1:(length(df) - 1)) {
    # for factor variable
    if (class(df[, i]) == "factor") {
      if (length(levels(df[, i])) < 10)
        #(next)() else stop("ERROR: The number of categories should be less than 10")
        (next)()
      else
        warning("WARNING: The number of categories should be less than 10",
                names(df)[i])
    }

    # for continuous variable: variable transformation
    # select discretization method, default mode = 1

    ## mode 1 - quantiles
    if (categorize == "quantile") {
      # options(scipen = 20)
      #print("in quantile")
      cut_off_tmp <- quantile(df[, i], quantiles)
      cut_off_tmp <- unique(cut_off_tmp)
      cut_off <- signif(cut_off_tmp, 3)  # remain 3 digits
      #print(cut_off)

      ## mode 2 k-means clustering
    } else if (categorize == "k_means") {
      #print("using k-means")
      clusters <- kmeans(df[, i], max_cluster)
      cut_off_tmp <- c()
      for (j in unique(clusters$cluster)) {
        #print(min(df[,i][clusters$cluster==j]))
        #print(length(df[,i][clusters$cluster==j]))
        cut_off_tmp <- append(cut_off_tmp, min(df[, i][clusters$cluster == j]))
        #print(cut_off_tmp)
      }
      cut_off_tmp <- append(cut_off_tmp, max(df[, i]))
      cut_off_tmp <- sort(cut_off_tmp)
      #print(names(df)[i])
      #assert (length(cut_off_tmp) == 6)
      cut_off_tmp <- unique(cut_off_tmp)
      cut_off <- signif(cut_off_tmp, 3)
      cut_off <- unique(cut_off)
      #print (cut_off)

    } else {
      stop('ERROR: please specify correct method for categorizing:  "quantile" or "k_means".')
    }

    l <- list(cut_off)
    names(l)[1] <- names(df)[i]
    cut_vec <- append(cut_vec, l)
    #print("****************************cut_vec*************************")
    #print(cut_vec)
  }
  for(i in 1:length(cut_vec)) cut_vec[[i]] <- cut_vec[[i]][2:(length(cut_vec[[i]]) - 1)]
  return(cut_vec)
}
#' @title Internal function: Categorizing continuous variables based on fixed cut_vec
#' @param df dataset to be processed
#' @param cut_vec fixed cut vector
#' @return  Processed \code{data.frame} after categorizing based on fixed cut_vec
transform_df_fixed <- function(df, cut_vec) {
  j <- 1

  # for loop going through all variables
  for (i in 1:(length(df) - 1)) {

    if (class(df[, i]) == "factor") {
      if (length(levels(df[, i])) < 10)
        (next)() else stop("ERROR: The number of categories should be less than 10")
    }

    ## make conresponding cutvec for validation_set: cut_vec_new
    vec<-df[, i]
    cut_vec_new <- cut_vec[[j]]
    if(min(vec) < cut_vec[[j]][1]) {
      cut_vec_new <- c(floor(min(df[, i]))-100, cut_vec_new)
    }
    if(max(vec) >= cut_vec[[j]][length(cut_vec[[j]])]) {
      cut_vec_new <- c(cut_vec_new, ceiling(max(df[, i])+100))
    }

    cut_vec_new_tmp <- signif(cut_vec_new, 3)
    cut_vec_new_tmp <- unique(cut_vec_new_tmp)  ###revised update##
    df[, i] <- cut(df[, i], breaks = cut_vec_new_tmp, right = F,
                   include.lowest = F, dig.lab = 3)
    # xmin<-as.character(min(cut_vec_new_tmp)) xmax<-as.character(max(cut_vec_new_tmp))

    ## delete min and max for the Interval after discretion: validation_set
    if(min(vec) < cut_vec[[j]][1]) {
      levels(df[, i])[1] <- gsub(".*,", "(,", levels(df[, i])[1])
    }
    if(max(vec) >= cut_vec[[j]][length(cut_vec[[j]])]) {
      levels(df[, i])[length(levels(df[, i]))] <- gsub(
        ",.*", ",)",
        levels(df[, i])[length(levels(df[, i]))]
      )
    }

    j <- j + 1
  }
  return(df)
}


#' @title Build-in Function: Change Reference category after first-step logistic regression
#' @param df A \code{data.frame} used for logistic regression
#' @param coef_vec Generated from logistic regression
#' @return Processed \code{data.frame} after changing reference category
change_reference <- function(df, coef_vec) {
  # delete label first
  df_tmp <- subset(df, select = -label)

  # one loops to go through all variable
  for (i in (1:length(df_tmp))) {
    var_name <- names(df_tmp)[i]
    var_levels <- levels(df_tmp[, i])
    var_coef_names <- paste0(var_name, var_levels)
    coef_i <- coef_vec[which(names(coef_vec) %in% var_coef_names)]
    # if min(coef_tmp)<0, the current lowest one will be used for reference
    if (min(coef_i) < 0) {
      ref <- var_levels[which(var_coef_names == names(coef_i)[which.min(coef_i)])]
      df_tmp[, i] <- relevel(df_tmp[, i], ref = ref)
    }
    # char_tmp <- paste("^", names(df_tmp)[i], sep = "")
    # coef_tmp <- coef_vec[grepl(char_tmp, names(coef_vec))]
    # coef_tmp <- coef_tmp[!is.na(coef_tmp)]

    # if min(coef_tmp)<0, the current lowest one will be used for reference
    # if (min(coef_tmp) < 0) {
    #   ref <- gsub(names(df_tmp)[i], "", names(coef_tmp)[which.min(coef_tmp)])
    #   df_tmp[, i] <- relevel(df_tmp[, i], ref = ref)
    # }
  }

  # add lable again
  df_tmp$label <- df$label
  return(df_tmp)
}


#' @title Internal Function: Add baselines after second-step logistic regression
#' @param df A \code{data.frame} used for logistic regression
#' @param coef_vec Generated from logistic regression
#' @return Processed \code{vector} for generating the scoring table
add_baseline <- function(df, coef_vec) { # Proposed new version
  df <- subset(df, select = -label)
  coef_names_all <- unlist(lapply(names(df), function(var_name) {
    paste0(var_name, levels(df[, var_name]))
  }))
  coef_vec_all <- numeric(length(coef_names_all))
  names(coef_vec_all) <- coef_names_all
  # Remove items in coef_vec that are not meant to be in coef_vec_all
  # (i.e., the intercept)
  coef_vec_core <- coef_vec[which(names(coef_vec) %in% names(coef_vec_all))]
  i_coef <- match(x = names(coef_vec_core), table = names(coef_vec_all))
  coef_vec_all[i_coef] <- coef_vec_core
  coef_vec_all
}


#' @title Internal Function: Automatically assign scores to each subjects given
#'   Test Set and scoring table.
#' @param df A \code{data.frame} used for testing, where variables keep before
#'   categorization
#' @param score_table A \code{vector} containing the scoring table
#' @return Processed \code{data.frame} with assigned scores for each variables
assign_score <- function(df, score_table) {
  for (i in 1:(length(names(df))-1)) {
    score_table_tmp <- score_table[grepl(names(df)[i], names(score_table))]
    df[, i] <- as.character(df[, i])
    for (j in 1:length(names(score_table_tmp))) {
      df[, i][df[, i] %in% gsub(names(df)[i], "", names(score_table_tmp)[j])] <-
        score_table_tmp[j]
    }

    df[, i] <- as.numeric(df[, i])
  }

  return(df)
}


#' @title Build-in Function: Categorizing continuous variables based on fixed cut_vec(AutoScore Module 2)
#' @param df dataset to be processed
#' @return  Processed \code{data.frame} after Categorizing based on fixed cut_vec
#' @seealso
transform_df_fixed <- function(df, cut_vec) {
  j <- 1

  # for loop going through all variables
  for (i in 1:(length(df) - 1)) {

    if (class(df[, i]) == "factor") {
      if (length(levels(df[, i])) < 10)
        (next)() else stop("Error!! The number of categories should be less than 9")
    }

    ## make conresponding cutvec for validation_set: cut_vec_new
    vec <- df[, i]
    cut_vec_new <- cut_vec[[j]]
    if(min(vec) < cut_vec[[j]][1]) cut_vec_new <- c(floor(min(df[, i]))*0.8, cut_vec_new)
    if(max(vec) >= cut_vec[[j]][length(cut_vec[[j]])] ) {
      cut_vec_new <- c(cut_vec_new, ceiling(max(df[, i])*1.2))
    }

    cut_vec_new_tmp <- signif(cut_vec_new, 3)
    cut_vec_new_tmp <- unique(cut_vec_new_tmp)  ###revised update##
    df[, i] <- cut(df[, i], breaks = cut_vec_new_tmp, right = F, include.lowest = F, dig.lab = 3)

    ## delete min and max for the Interval after discretion: validation_set
    if(min(vec) < cut_vec[[j]][1]) levels(df[, i])[1] <- gsub(".*,", "(,", levels(df[, i])[1])
    if(max(vec) >= cut_vec[[j]][length(cut_vec[[j]])] ) levels(df[, i])[length(levels(df[, i]))] <- gsub(",.*", ",)", levels(df[, i])[length(levels(df[, i]))])
    j <- j + 1
  }
  return(df)
}

# CLM specific ------------------------------------------------------------

inv_logit <- function(x) {
  exp(x) / (1 + exp(x))
}
inv_cloglog <- function(x) {
  1 - exp(-exp(x))
}
inv_loglog <- function(x) {
  exp(-exp(-x))
}
inv_probit <- function(x) {
  pnorm(x)
}
estimate_p_mat <- function(theta, z, link) {
  n <- length(z)
  inv_link <- get(paste0("inv_", link))
  cump_mat <- inv_link(matrix(rep(theta, n), nrow = n, byrow = TRUE) -
                         matrix(rep(z, length(theta)), nrow = n, byrow = FALSE))
  cump_mat <- cbind(0, cump_mat, 1)
  t(apply(cump_mat, 1, diff))
}
#' Compute the cutoff values for X%*%beta from theta terms of a CLM
#' @param theta theta terms of a CLM. For an outcome with J categories, there
#'   should be J-1 elements in \code{theta}.
#' @param z A numeric vector for the predictor value. Make sure the range is
#'   sufficient for all intersection between probability curves to intersect.
#'   Default is \code{NULL} for \code{link = "logit"}.
#' @param link Link function of the CLM. Possible values are \code{"logit"}
#'   (default), \code{"probit"}, \code{"cloglog"} and \code{"loglog"}.
#' @return Returns a numeric vector of length J-1.
#' @import reconPlots
#' @export
compute_cutoffs <- function(theta, z = NULL, link) {
  link <- match.arg(arg = link, choices = c("logit", "probit", "cloglog", "loglog"))
  if (link == "logit") {
    J <- length(theta) + 1
    cutoff <- NULL
    if (J > 3) {# Check for bug
      cutoff <- unlist(lapply(2:(J - 2), function(j) {
        log((2 * exp(theta[j - 1] + theta[j + 1]) -
               exp(theta[j - 1] + theta[j]) -
               exp(theta[j] + theta[j + 1])) /
              (2 * exp(theta[j]) - exp(theta[j - 1]) - exp(theta[j + 1])))
      }))
    }
    sort(c(theta[1] + theta[2] - log(exp(theta[2]) - 2 * exp(theta[1])),
           cutoff,
           log(exp(theta[J - 1]) - 2 * exp(theta[J - 2]))))
  } else {
    if (is.null(z)) stop(simpleError("z is required when link is not logit."))
    p_mat <- estimate_p_mat(theta = theta, z = z, link = link)
    unlist(lapply(1:(ncol(p_mat) - 1), function(j) {
      reconPlots::curve_intersect(curve1 = data.frame(x = z, y = p_mat[, j]),
                                  curve2 = data.frame(x = z, y = p_mat[, j + 1]),
                                  empirical = TRUE)$x
    }))
  }
}
#' Optimise thresholds for total score using cumulative ROC analyses
#' @param y Observed ordinal outcome.
#' @param total_score Total score generated from AutoScore framework.
#' @import pROC
optimise_thresholds <- function(y, total_score) {
  y <- as.numeric(ordered(y))
  J <- length(unique(y))
  do.call("rbind", lapply(1:(J - 1), function(j) {
    # ROC analysis of {1, ..., j} vs {j+1, ..., J}
    y_j <- as.numeric(y <= j)
    roc_j <- pROC::roc(response = y_j, predictor = total_score)
    auc_ci_j <- pROC::ci(roc_j)
    metric_j <- pROC::coords(
      roc = roc_j, x = "best", best.method = "closest.topleft",
      ret = c("threshold", "specificity", "sensitivity", "npv", "ppv"),
      transpose = TRUE
    )
    as.data.frame(as.list(c(
      j = j, auc = auc_ci_j[2], auc_ci_lower = auc_ci_j[1],
      auc_ci_upper = auc_ci_j[3], metric_j
    )))
  }))
}
